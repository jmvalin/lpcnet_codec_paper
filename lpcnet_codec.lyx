#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble


\usepackage{INTERSPEECH2019}

\usepackage[pdftex,linkcolor=black,urlcolor=black,citecolor=black,pdfpagemode=None,pdfstartview=FitH,pdfview=FitH,colorlinks=true,pdftitle=LPCNet: Improving Neural Speech Synthesis Through Linear Prediction,pdfauthor=Jean-Marc Valin]{hyperref}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title{A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet}
\end_layout

\begin_layout Plain Layout


\backslash
name{Jean-Marc Valin$^1$, Jan Skoglund$^2$}
\end_layout

\begin_layout Plain Layout


\backslash
address{   $^1$Mozilla, Mountain View, CA, USA
\backslash

\backslash
   $^2$Google LLC, San Francisco, CA, USA}
\end_layout

\begin_layout Plain Layout


\backslash
email{jmvalin@jmvalin.ca, jks@google.com}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset

 
\end_layout

\begin_layout Abstract
We present a low-bitrate codec based on the LPCNet model.
 The use of linear prediction and sparse matrices makes it possible to achieve
 real-time operation on general-purpose hardware.
\end_layout

\begin_layout Abstract
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{3cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\series bold
Index Terms
\series default
: neural speech synthesis, wideband coding, vocoder, LPCNet, WaveRNN
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Low complexity parametric codecs have existed for a long time
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "atal1971speech,markel1974linear"
literal "false"

\end_inset

, but their quality has always been severely limited.
 While they are generally efficient at modeling the spectral envelope (vocal
 tract response) of the speech using linear prediction, no such simple model
 exists for the excitation.
 Despite some advances
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "griffin1985new,mccree1996,rowe1997"
literal "false"

\end_inset

, modeling the excitation signal has remained a challenge.
\end_layout

\begin_layout Standard
Neural speech synthesis algorithms such as Wavenet
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "van2016wavenet"
literal "false"

\end_inset

 and SampleRNN
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "mehri2016samplernn"
literal "false"

\end_inset

 have recently made it possible to synthesize high quality speech.
 They have also be used in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kleijn2018wavenet,Garbacea2019"
literal "false"

\end_inset

 (WaveNet) and
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

 (SampleRNN) to synthesize high-quality speech from coded features, with
 a complexity ranging in the tens of GFLOPS.
 This typically makes it impossible to use those algorithms in real time
 without high-end hardware (if at all).
 In this work, we focus on simpler models, that are can be implemented on
 general-purpose hardware and mobile devices for real-time communication,
 and that work for any speaker using any language.
 Moreover, we target the very low bitrate of 1.6
\begin_inset space ~
\end_inset

kb/s, which is beyond the reach of conventional waveform speech coders.
 
\end_layout

\begin_layout Standard
WaveRNN
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kalchbrenner2018efficient"
literal "false"

\end_inset

 is a more computationally efficient alternative to WaveNet that uses a
 recurrent neural network (RNN) along with sparse matrices.
 Another way of reducing the complexity is to use linear prediction as a
 way of taking the burden of spectral envelope modeling away from the neural
 synthesis network
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet,juvela2018speaker,wang2019"
literal "false"

\end_inset

.
 In previous work, we proposed LPCNet
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

 (summarized in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:WaveRNN_and_LPCNet"
plural "false"
caps "false"
noprefix "false"

\end_inset

), which augments WaveRNN with linear prediction to achieve low complexity
 speaker-independent speech synthesis.
 We now address how to quantize LPCNet features to achieve low-bitrate speech
 coding (Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Features_and_quantization"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and then discuss training considerations in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Training"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We provide results in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Evaluation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and conclude with ideas for improvement in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Conclusion"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
LPCNet Overview
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:WaveRNN_and_LPCNet"

\end_inset


\end_layout

\begin_layout Standard
The LPCNet model, summarized in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-LPCNet"
plural "false"
caps "false"
noprefix "false"

\end_inset

, operates on signals quantized using 256-level 
\begin_inset Formula $\mu$
\end_inset

\SpecialChar nobreakdash
law.
 To avoid audible quantization noise, a pre-emphasis filter 
\begin_inset Formula $E(z)=1-\alpha z^{-1}$
\end_inset

 is used on the input speech (with 
\begin_inset Formula $\alpha=0.85$
\end_inset

) and the inverse (de-emphasis) filter is applied to the output.
 This shapes the noise and makes it less perceptible.
 In addition to using the previously generated speech sample 
\begin_inset Formula $s_{t-1}$
\end_inset

, LPCNet also uses the current prediction 
\begin_inset Formula $p_{t}$
\end_inset

 and the previously generated excitation 
\begin_inset Formula $e_{t-1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Like WaveRNN, LPCNet is based on a gated recurrent unit (GRU)
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "cho2014properties"
literal "false"

\end_inset

 layer.
 Considering that the signal 
\begin_inset Formula $s_{t-1}$
\end_inset

, prediction 
\begin_inset Formula $p_{t}$
\end_inset

 and excitatation 
\begin_inset Formula $e_{t-1}$
\end_inset

 inputs are discrete, we can pre-compute the contribution 
\begin_inset Formula $\mathbf{v}_{i}^{\left(\cdot,\cdot\right)}$
\end_inset

 of each possible value to the 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 gates and state so that only lookups are necessary at run-time.
 In addition, the contribution 
\begin_inset Formula $\mathbf{g}^{\left(\cdot\right)}$
\end_inset

 of the frame rate network to 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 can be computed only once per frame.
 
\begin_inset Formula 
\begin{align}
\mathbf{u}_{t}= & \sigma\left(\mathbf{W}_{u}\mathbf{h}_{t-1}+\mathbf{v}_{s_{t-1}}^{\left(u,s\right)}+\mathbf{v}_{p_{t}}^{\left(u,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(u,e\right)}+\mathbf{g}^{\left(u\right)}\right)\nonumber \\
\mathbf{r}_{t}= & \sigma\left(\mathbf{W}_{r}\mathbf{h}_{t-1}+\mathbf{v}_{s_{t-1}}^{\left(r,s\right)}+\mathbf{v}_{p_{t}}^{\left(r,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(r,e\right)}+\mathbf{g}^{\left(r\right)}\right)\label{eq:LPCNet}\\
\widetilde{\mathbf{h}}_{t}= & \tau\left(\mathbf{r}_{t}\circ\left(\mathbf{W}_{h}\mathbf{h}_{t-1}\right)+\mathbf{v}_{s_{t-1}}^{\left(h,s\right)}+\mathbf{v}_{p_{t}}^{\left(h,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(h,e\right)}+\mathbf{g}^{\left(h\right)}\right)\nonumber \\
\mathbf{h}_{t}= & \mathbf{u}_{t}\circ\mathbf{h}_{t-1}+\left(1-\mathbf{u}_{t}\right)\circ\widetilde{\mathbf{h}}_{t}\nonumber \\
P\left(e_{t}\right) & =\mathrm{softmax}\left(\mathrm{dual\_fc}\left(\mathrm{GRU_{B}}\left(\mathbf{h}_{t}\right)\right)\right)\ ,\nonumber 
\end{align}

\end_inset

where the 
\begin_inset Formula $\mathbf{W}$
\end_inset

 matrices are the 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 weights, 
\begin_inset Formula $\sigma\left(x\right)$
\end_inset

 is the sigmoid function, 
\begin_inset Formula $\tau\left(x\right)$
\end_inset

 is the hyperbolic tangent, 
\begin_inset Formula $\circ$
\end_inset

 denotes an element-wise vector multiply, and 
\begin_inset Formula $\mathrm{GRU_{A}}\left(\cdot\right)$
\end_inset

 is a regular (non-sparse) GRU.
 The dual fully-connected (dual_fc) layer is defined as
\begin_inset Formula 
\begin{equation}
\mathrm{dual\_fc}(\mathbf{x})=\mathbf{a}_{1}\circ\tau\left(\mathbf{W}_{1}\mathbf{x}\right)+\mathbf{a}_{2}\circ\tau\left(\mathbf{W}_{2}\mathbf{x}\right)\ ,\label{eq:dual_fc}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{W}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}_{2}$
\end_inset

 are weight matrices and 
\begin_inset Formula $\mathbf{a}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{a}_{2}$
\end_inset

 are scaling vectors.
\end_layout

\begin_layout Standard
Throughout this paper, biases are omitted for clarity.
 The synthesized output samples 
\begin_inset Formula $s_{t}$
\end_inset

 is obtained by sampling from the probability distribution 
\begin_inset Formula $P\left(s_{t}\right)$
\end_inset

 after lowering the temperature of voiced frames as described in eq.
\begin_inset space ~
\end_inset

(7) of 
\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

.
 As a way of reducing the complexity, the 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 uses sparse recurrent matrics with non-zero blocks of size 16x1 to ensure
 efficient vectorization.
 Because the hidden state update is more important then the reset and update
 gates, we keep 20% of the weights in 
\begin_inset Formula $\mathbf{W}_{h}$
\end_inset

, but only 5% of those in 
\begin_inset Formula $\mathbf{W}_{r}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}_{u}$
\end_inset

, for an average of 10%.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename overview.pdf
	width 100col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the LPCNet model.
 The frame rate network (yellow) operates on 10\SpecialChar nobreakdash
ms frames and its output
 is held constant through each frame for the sample rate network (blue).
 The 
\emph on
compute prediction
\emph default
 block applies linear prediction to predict the sample at time 
\begin_inset Formula $t$
\end_inset

 from the previous samples.
 Conversions between 
\begin_inset Formula $\mu$
\end_inset

-law and linear are omitted for clarity.
 The de-emphasis filter is applied to the output 
\begin_inset Formula $s_{t}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-LPCNet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Features and Quantization
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Features_and_quantization"

\end_inset


\end_layout

\begin_layout Standard
LPCNet is designed to operate with 10\SpecialChar nobreakdash
ms frames containing 20
\begin_inset space ~
\end_inset

features.
 Each frame includes 18
\begin_inset space ~
\end_inset

cepstral coefficients, a pitch period (between 16 and 256
\begin_inset space ~
\end_inset

samples) and a pitch correlation (between 0 and 1).
 To achieve a low bitrate, we use 40\SpecialChar nobreakdash
ms packets, each representing 4
\begin_inset space ~
\end_inset

LPCNet frames.
 Each packet is coded with 64
\begin_inset space ~
\end_inset

bits, allocated as shown in Table
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Bit-allocation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, for a total bitrate of 1.6
\begin_inset space ~
\end_inset

kb/s (CBR).
 The prediction coefficients are computed using the quantized cepstrum.
\end_layout

\begin_layout Subsection
Pitch
\end_layout

\begin_layout Standard
Extracting the correct pitch (without period doubling or halving) is very
 important for a vocoder since no residual is coded to make up for prediction
 errors.
 During development, we have observed that unlike traditional vocoders,
 the neural vocoder has some ability to compensate for incorrect pitch values,
 but only up to a point.
 Moreover, that ability is reduced when the cepstrum is quantized.
 
\end_layout

\begin_layout Standard
The pitch search operates on the excitation signal.
 Maximizing the correlation over an entire 40\SpecialChar nobreakdash
ms packet does not produce
 good results because the pitch can vary within that time.
 Instead, we divide each packet in 5\SpecialChar nobreakdash
ms sub-frames, and find the set of pitch
 lags 
\begin_inset Formula $\tau_{i}$
\end_inset

 that maximize the following expression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J=\sum_{i}\left[w_{i}r\left(\tau_{i}\right)+\Theta\left(\tau_{i}-\tau_{i-1}\right)\right]\,,
\]

\end_inset

where 
\begin_inset Formula $w_{i}$
\end_inset

 is the ratio of the sub-frame energy over the average energy over the 40\SpecialChar nobreakdash
ms
 packet, 
\begin_inset Formula $\Theta\left(\Delta_{\tau}\right)$
\end_inset

 is a transition penalty, and 
\begin_inset Formula $r\left(\tau_{i}\right)$
\end_inset

 is a modified pitch correlation.
 
\end_layout

\begin_layout Standard
The optimal path can be computed using dynamic programming with a Viterbi
 search.
 Since the entire audio is not available at once, the running values of
 
\begin_inset Formula $J$
\end_inset

 are updated for every sub-frame and the Viterbi backtrack pass is computed
 once per 40\SpecialChar nobreakdash
ms packet, over all 8
\begin_inset space ~
\end_inset

sub-frames.
 While that does not guarantee finding the optimal path, it ensures a consistent
 pitch over the duration of the packet.
 
\end_layout

\begin_layout Standard
The allowed pitch values range from 62.5
\begin_inset space ~
\end_inset

Hz to 500
\begin_inset space ~
\end_inset

Hz.
 The pitch is encoded on a logarithmic scale using 6
\begin_inset space ~
\end_inset

bits, resulting in quantization intervals of 0.57
\begin_inset space ~
\end_inset

semitones.
 
\end_layout

\begin_layout Standard
A linear pitch modulation parameter allows up to 16% variation (2.5
\begin_inset space ~
\end_inset

semitones) between the first and last sub-frame.
 It is encoded with 3 bits representing a 
\begin_inset Formula $\left[-3,3\right]$
\end_inset

 range with the two different codes for 0 (constant modulation), one of
 which also signals that the pitch correlation is less than 0.3 (the modulation
 is zero when the pitch correlation is too small).
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bit allocation for a 40\SpecialChar nobreakdash
ms frame.
\begin_inset CommandInset label
LatexCommand label
name "tab:Bit-allocation"

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bits
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch period
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch modulation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch gain
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Energy (C0)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum VQ (40
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum delta (20
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum interpolation (10
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Total
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cepstrum
\end_layout

\begin_layout Standard
The spectral analysis operates on 20\SpecialChar nobreakdash
ms windows with 50% overlap.
 The cepstrum is computed from 18
\begin_inset space ~
\end_inset

Bark-spaced bands following the layout in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2017hybrid"
literal "false"

\end_inset

.
 Because we pack 4
\begin_inset space ~
\end_inset

cepstral vectors in each packet, we wish to maximize redundancy elimination
 within a packet while limiting dependencies across packets to reduce the
 effect of packet loss.
 For those reasons, we use a quantization scheme inspired from video codec
 B\SpecialChar nobreakdash
frames
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "MPEG1"
literal "false"

\end_inset

, limiting the error propagation in case of packet loss to a worst case
 of 30
\begin_inset space ~
\end_inset

ms.
 It is illustrated in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Prediction-and-quantization"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Let packet 
\begin_inset Formula $k$
\end_inset

 include cepstral vectors 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 to 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

, we first code all but the first component of 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

 independently for each packet using a 3\SpecialChar nobreakdash
stage codebook with 10
\begin_inset space ~
\end_inset

bits for each stage.
 An M-best (survivor) search helps reduce the quantization error, but is
 not strictly necessary.
 The first component (C0) of 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

 is coded with a uniform 7\SpecialChar nobreakdash
bit scalar quantizer (0.83
\begin_inset space ~
\end_inset

dB resolution).
 From there, vector 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

 is predictively coded from both 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 (independently coded in the previous packet) and 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 We use a single bit to signal if the prediction is the average (
\begin_inset Formula $\frac{\mathbf{c}_{4k-1}+\mathbf{c}_{4k+3}}{2}$
\end_inset

), or two bits if the prediction is either of 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 or 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 The difference is then coded with a 12\SpecialChar nobreakdash
bit + sign codebook for the average
 or with an 11\SpecialChar nobreakdash
bit + sign codebook if not, for a total of 14
\begin_inset space ~
\end_inset

bits 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

.
 Although the average predictor is the most useful, including the single-vector
 predictors improves the quantization of transients/onsets.
 
\end_layout

\begin_layout Standard
Because there is insufficient bitrate to adequately code 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+2}$
\end_inset

, we only use a prediction from their neighbours.
 Vector 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 is predicted from its neighbours 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

, whereas 
\begin_inset Formula $\mathbf{c}_{4k+2}$
\end_inset

 is predicted from 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 Since there are 3
\begin_inset space ~
\end_inset

options for each vector, we have 9
\begin_inset space ~
\end_inset

possible combinations.
 By eliminating the least useful combination (
\begin_inset Formula $\mathbf{c}_{4k}=\mathbf{c}_{4k+1}=\mathbf{c}_{4k+2}$
\end_inset

), we can code the remaining ones with 3
\begin_inset space ~
\end_inset

bits.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename quantization.pdf
	width 90col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Prediction and quantization of the cepstrum for packet
\begin_inset space ~
\end_inset


\begin_inset Formula $k$
\end_inset

.
 Vectors in green are quantized independently, vectors in blue are quantized
 with prediction, and vectors in red use prediction with no residual quantizatio
n.
 Prediction is shown by the arrows.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Prediction-and-quantization"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Training
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Training"

\end_inset


\end_layout

\begin_layout Standard
When decoding speech, the sample rate network operates on the synthized
 speech rather than on the 
\emph on
ground-truth
\emph default
 clean speech.
 That mismatch between training and inference can increase the speech distortion
, in turn increasing the mismatch.
 To make the network more robust to the mismatch, we add noise to the training
 data, as originally suggested in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "jin2018fftnet"
literal "false"

\end_inset

.
 As an improvement over our previous work
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

, the noise is added to the excitation inside the prediction loop, making
 the signal and excitation noise consistent with each other, as shown in
 Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Noise-injection"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
As a way of making the model more robust to input variations, we use data
 augmentation on the training database.
 The speech level is varied over a 40
\begin_inset space ~
\end_inset

dB range and the frequency response is varied according to eq.
\begin_inset space ~
\end_inset

(7) in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2017hybrid"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename training_noise.pdf
	width 100col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Noise injection during the training procedure, with 
\begin_inset Formula $Q$
\end_inset

 denoting 
\begin_inset Formula $\mu$
\end_inset

-law quantization and 
\begin_inset Formula $Q^{-1}$
\end_inset

 denoting conversion from 
\begin_inset Formula $\mu$
\end_inset

-law to linear.
 The prediction filter filter is given by 
\begin_inset Formula $P\left(z\right)=\sum_{k=1}^{M}a_{k}z^{-k}$
\end_inset

.
 The target excitation is computed as the difference between the clean,
 unquantized input and the noisy prediction.
 Note that the noise is added in the 
\begin_inset Formula $\mu$
\end_inset

-law domain so that its power follows that of the real excitation signal.
\begin_inset CommandInset label
LatexCommand label
name "fig:Noise-injection"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Training is first performed with unquantized features.
 When training a model for quantized features, we start with a model trained
 on unquantized features, and apply domain adaptation to quantized features.
 We have observed that better results are obtained when only the frame rate
 network is adapted, with the sample rate network weights left unchanged.
 In addition to the slightly better quality, this has the advantage of faster
 training for new quantizers and also smaller storage if different quantized
 models are needed.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
The source code for this work is available under a BSD license at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://github.com/mozilla/LPCNet/}
\end_layout

\end_inset

.
 The evaluation in this section is based on commit 0123456789abcdef.
\end_layout

\begin_layout Subsection
Complexity and Implementation
\end_layout

\begin_layout Standard
The number of weights in the sample rate network is approximately
\begin_inset Formula 
\[
W=3dN_{A}^{2}+3N_{B}\left(N_{A}+N_{B}\right)+2N_{B}Q\,,
\]

\end_inset

where 
\begin_inset Formula $N_{A}$
\end_inset

 and 
\begin_inset Formula $N_{B}=16$
\end_inset

 are the sizes of the two GRUs, 
\begin_inset Formula $d$
\end_inset

 is the density of the sparse GRU, 
\begin_inset Formula $Q=256$
\end_inset

 is the number of 
\begin_inset Formula $\mu$
\end_inset

-law levels.
 Based on the subjective results in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

, we consider 
\begin_inset Formula $N_{A}=384$
\end_inset

 with 
\begin_inset Formula $d=10\%$
\end_inset

 (122
\begin_inset space ~
\end_inset

dense equivalent units) to provide a good compromise between quality and
 complexity.
 This results in 
\begin_inset Formula $W=71600\,\mathrm{weights}$
\end_inset

, which fits in the L2 or L3 cache of most modern CPUs.
 Considering that each weight is used once per sample for a multiply-add,
 the resulting complexity is 2.3
\begin_inset space ~
\end_inset

GFLOPS.
 The activation functions are based on a vectorized exponential approximation
 and contribute 0.6
\begin_inset space ~
\end_inset

GFLOPS to the complexity, for a total of 3
\begin_inset space ~
\end_inset

GFLOPS when counting the remaining operations.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
exp=15, tanh=19, sig=18, softmax=21
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A C implementation of the decoder (with AVX2/FMA intrinsics) requires 20%
 of a 2.4
\begin_inset space ~
\end_inset

GHz Intel Broadwell core for real-time decoding (5x faster than real-time).
 According to our analysis, the main performance bottleneck is the L2 cache
 bandwidth required for the matrix-vector products.
 On ARMv8 (with Neon intrinsics), real-time decoding on a 2.5
\begin_inset space ~
\end_inset

GHz Snapdragon
\begin_inset space ~
\end_inset

845 (Google Pixel
\begin_inset space ~
\end_inset

3) requires 68% of one core (1.47x
\begin_inset space ~
\end_inset

real-time).
 On the more recent 2.84
\begin_inset space ~
\end_inset

GHz Snapdragon
\begin_inset space ~
\end_inset

855 (Samsung Galaxy
\begin_inset space ~
\end_inset

S10), real-time decoding requires only 31% of one core
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Both the Snapdragon
\begin_inset space ~
\end_inset

845 and 855 are big.LITTLE architectures and we were unable to control or
 identify which core was being used.
\end_layout

\end_inset

 (3.2x
\begin_inset space ~
\end_inset

real-time).
\end_layout

\begin_layout Standard
As a comparison, we estimate the complexity of the SampleRNN neural codec
 described in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

 to be around 100
\begin_inset space ~
\end_inset

GFLOPS – mostly from the two-hidden-layer MLPs with 1024 units per layer.
 The complexity of the WaveNet-based codec in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kleijn2018wavenet"
literal "false"

\end_inset

 significantly exceeds 100
\begin_inset space ~
\end_inset

GFLOPS
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
based on discussion with the authors
\end_layout

\end_inset

.
 For speaker-dependent text-to-speech
\begin_inset space ~
\end_inset

(TTS) – which typically allows smaller models – real-time synthesis was
 achieved by
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "arik2017deep_rt"
literal "false"

\end_inset

 (WaveNet) and
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kalchbrenner2018efficient"
literal "false"

\end_inset

 (WaveRNN) when using multiple CPU cores.
\end_layout

\begin_layout Standard
The total complexity of the proposed encoder is around 30
\begin_inset space ~
\end_inset

MFLOPS (0.03
\begin_inset space ~
\end_inset

GFLOPS), mostly from the 5-best VQ search (14
\begin_inset space ~
\end_inset

MFLOPS) and the undecimated 16
\begin_inset space ~
\end_inset

kHz pitch search (8
\begin_inset space ~
\end_inset

MFLOPS).
 Although the encoder complexity could be significantly reduced, it is already
 only 1% of the decoder complexity.
\end_layout

\begin_layout Subsection
Experimental Setup
\end_layout

\begin_layout Standard
The model is trained using 4
\begin_inset space ~
\end_inset

hours of speech from the NTT Multi-Lingual Speech Database for Telephonometry
 (21
\begin_inset space ~
\end_inset

languages), from which we excluded all samples from the speakers used in
 testing.
 By varying the level and frequency response, we generate 14 hours of augmented
 speech data.
 The unquantized network was trained for 120
\begin_inset space ~
\end_inset

epochs (625k
\begin_inset space ~
\end_inset

updates), with a batch size size of 64, each sequence consisting of 2400
\begin_inset space ~
\end_inset

samples (15
\begin_inset space ~
\end_inset

frames).
 Training was performed on an Nvidia GPU with Keras
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://keras.io/}
\end_layout

\end_inset


\end_layout

\end_inset

/Tensorflow
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://www.tensorflow.org/}
\end_layout

\end_inset


\end_layout

\end_inset

 using the CuDNN GRU implementation and the AMSGrad
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "reddi2018convergence"
literal "false"

\end_inset

 optimization method (Adam variant) with a step size 
\begin_inset Formula $\alpha=\frac{\alpha_{0}}{1+\delta\cdot b}$
\end_inset

 where 
\begin_inset Formula $\alpha_{0}=0.001$
\end_inset

, 
\begin_inset Formula $\delta=5\times10^{-5}$
\end_inset

, and 
\begin_inset Formula $b$
\end_inset

 is the batch number.
 For model adaptation with quantized features, we used 40
\begin_inset space ~
\end_inset

epochs (208k
\begin_inset space ~
\end_inset

updates) with 
\begin_inset Formula $\alpha_{0}=0.0001$
\end_inset

, 
\begin_inset Formula $\delta=0$
\end_inset

.
\end_layout

\begin_layout Subsection
Quality
\end_layout

\begin_layout Standard
We conducted a subjective listening test with a MUSHRA-derived methodology
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "BS1534"
literal "false"

\end_inset

 to evaluate the quality of the proposed 1.6
\begin_inset space ~
\end_inset

kb/s neural vocoder.
 As an upper bound on the quality achievable with LPCNet at the target complexit
y (higher quality is achievable with a larger model), we include LPCNet
 operating on unquantized features.
 We also compare with Opus
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "vos2013voice"
literal "false"

\end_inset

 wideband (SILK mode) operating at 9
\begin_inset space ~
\end_inset

kb/s VBR (the lowest bitrate for which the encoder defaults to wideband)
 and with the narrowband MELP
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "mccree1996"
literal "false"

\end_inset

 vocoder.
 As low anchor, we use Speex
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2007speex"
literal "false"

\end_inset

 operating as a 4
\begin_inset space ~
\end_inset

kb/s wideband vocoder (wideband quality
\begin_inset space ~
\end_inset

0).
\end_layout

\begin_layout Standard
In a first test, we used 8
\begin_inset space ~
\end_inset

samples from 2
\begin_inset space ~
\end_inset

male and 2
\begin_inset space ~
\end_inset

female speakers.
 The samples are part of the NTT database used for training, but all samples
 from the selected speakers were excluded from the training set.
 As reported in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

, mismatches between the training and testing database can cause a significant
 difference in the output quality.
 To measure that impact, we test the same model on 8
\begin_inset space ~
\end_inset

samples (one
\begin_inset space ~
\end_inset

male and one
\begin_inset space ~
\end_inset

female speaker) from the sample set used to create the Opus testvectors.
 
\end_layout

\begin_layout Standard
The results presented in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Subjective-quality-MUSHRA"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show that...
\end_layout

\begin_layout Standard
Since the LPCNet model was trained on 21
\begin_inset space ~
\end_inset

languages, it is expected to also work in those languages.
 While we only tested on English, informal listening indicates that the
 quality obtained on French is comparable to that on English.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{4cm}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Subjective quality (MUSHRA) results for NTT samples.
\begin_inset CommandInset label
LatexCommand label
name "fig:Subjective-quality-MUSHRA"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{4cm}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Subjective quality (MUSHRA) results for testvector samples.
\begin_inset CommandInset label
LatexCommand label
name "fig:Subjective-quality-testvectors"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\end_layout

\begin_layout Standard
We have demonstrated a 1.6
\begin_inset space ~
\end_inset

kb/s neural vocoder that can be used for real-time communication on a mobile
 device.
 The quality obtained exceeds what is achievable in existing low-bitrate
 vocoders.
 
\end_layout

\begin_layout Standard
Considering that LPCNet is able to achieve higher quality than a waveform
 codec like Opus operating at low bitrate, we believe it should be possible
 to directly use the encoded LSPs to synthesize a higher quality output
 than the standard provides.
 In addition, using the encoded excitation as an extra input to LPCNet may
 help further improve the quality, turning LPCNet into a form of neural
 post-filter that can significantly improve the quality of any existing
 low-bitrate waveform codec.
 
\end_layout

\begin_layout Section
Acknowledgements
\end_layout

\begin_layout Standard
We'd like to acknowledge that there are acknowledgements.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "lpcnet"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
