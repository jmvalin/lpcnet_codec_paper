#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble


\usepackage{INTERSPEECH2019}

\usepackage[pdftex,linkcolor=black,urlcolor=black,citecolor=black,pdfpagemode=None,pdfstartview=FitH,pdfview=FitH,colorlinks=true,pdftitle=LPCNet: Improving Neural Speech Synthesis Through Linear Prediction,pdfauthor=Jean-Marc Valin]{hyperref}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title{A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet}
\end_layout

\begin_layout Plain Layout


\backslash
name{Jean-Marc Valin$^1$, Jan Skoglund$^2$}
\end_layout

\begin_layout Plain Layout


\backslash
address{   $^1$Mozilla, Mountain View, CA, USA
\backslash

\backslash
   $^2$Google LLC, San Francisco, CA, USA}
\end_layout

\begin_layout Plain Layout


\backslash
email{jmvalin@jmvalin.ca, jks@google.com}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset

 
\end_layout

\begin_layout Abstract
Neural speech synthesis algorithms are a promising new approach for coding
 speech at very low bitrate.
 They have so far demonstrated quality that far exceeds traditional vocoders,
 at the cost of very high complexity.
 In this work, we present a low-bitrate neural vocoder based on the LPCNet
 model.
 The use of linear prediction and sparse recurrent networks makes it possible
 to achieve real-time operation on general-purpose hardware.
 We demonstrate that LPCNet operating at 1.6
\begin_inset space ~
\end_inset

kb/s achieves significantly higher quality than MELP and that uncompressed
 LPCNet can exceed the quality of a waveform codec operating at low bitrate.
 This opens the way for new codec designs based on neural synthesis models.
\end_layout

\begin_layout Standard
\noindent

\series bold
Index Terms
\series default
: neural speech synthesis, wideband coding, vocoder, LPCNet, WaveRNN
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Very low bitrate parametric codecs have existed for a long time
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "atal1971speech,markel1974linear"
literal "false"

\end_inset

, but their quality has always been severely limited.
 While they are efficient at modeling the spectral envelope (vocal tract
 response) of the speech using linear prediction, no such simple model exists
 for the excitation.
 Despite some advances
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "griffin1985new,mccree1996,rowe1997"
literal "false"

\end_inset

, modeling the excitation signal has remained a challenge.
 For that reason, parametric codecs are rarely used above 3
\begin_inset space ~
\end_inset

kb/s.
 
\end_layout

\begin_layout Standard
Neural speech synthesis algorithms such as Wavenet
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "van2016wavenet"
literal "false"

\end_inset

 and SampleRNN
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "mehri2016samplernn"
literal "false"

\end_inset

 have recently made it possible to synthesize high quality speech.
 They have also been used in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kleijn2018wavenet,Garbacea2019"
literal "false"

\end_inset

 (WaveNet) and
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

 (SampleRNN) to synthesize high-quality speech from coded features, with
 a complexity in the order of 100
\begin_inset space ~
\end_inset

GFLOPS.
 This typically makes it impossible to use those algorithms in real time
 without high-end hardware (if at all).
 In this work, we focus on simpler models, that can be implemented on general-pu
rpose hardware and mobile devices for real-time communication, and that
 work for any speaker, in any language.
 Moreover, we target the very low bitrate of 1.6
\begin_inset space ~
\end_inset

kb/s, which is beyond the reach of conventional waveform speech coders.
 
\end_layout

\begin_layout Standard
To reduce computational complexity, WaveRNN
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kalchbrenner2018efficient"
literal "false"

\end_inset

 uses a sparse recurrent neural network (RNN).
 Other models use linear prediction to remove the burden of spectral envelope
 modeling from the neural synthesis network
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet,juvela2018speaker,wang2019"
literal "false"

\end_inset

.
 That includes our previous LPCNet work
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

 (summarized in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:WaveRNN_and_LPCNet"
plural "false"
caps "false"
noprefix "false"

\end_inset

), which augments WaveRNN with linear prediction to achieve low complexity
 speaker-independent speech synthesis.
\end_layout

\begin_layout Standard
We now address quantization of the LPCNet features to achieve low-bitrate
 speech coding (Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Features_and_quantization"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 discusses training considerations, and Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Evaluation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents our results.
 We conclude with ideas for improvement in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Conclusion"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
LPCNet Overview
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:WaveRNN_and_LPCNet"

\end_inset


\end_layout

\begin_layout Standard
The WaveRNN model
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kalchbrenner2018efficient"
literal "false"

\end_inset

 is based on a sparse gated recurrent unit (GRU)
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "cho2014properties"
literal "false"

\end_inset

 layer.
 At time 
\begin_inset Formula $t$
\end_inset

, it uses the previous audio sample 
\begin_inset Formula $s_{t-1}$
\end_inset

, as well as frame conditioning parameters to generates a discrete probability
 distribution 
\begin_inset Formula $P\left(s_{t}\right)$
\end_inset

 from which the output 
\begin_inset Formula $s_{t}$
\end_inset

 is sampled.
 LPCNet
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

 improves on WaveRNN by adding linear prediction, as shown in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-LPCNet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It is divided in two parts: a frame rate network that computes conditioning
 features for each 10\SpecialChar nobreakdash
ms frame, and a sample rate network that computes condition
al sample probabilities.
 In addition to using the previously generated speech sample 
\begin_inset Formula $s_{t-1}$
\end_inset

, LPCNet also uses the 
\begin_inset Formula $16^{th}$
\end_inset

 order prediction 
\begin_inset Formula $p_{t}=\sum_{i=1}^{16}a_{i}s_{t-i}$
\end_inset

 and the previously generated excitation 
\begin_inset Formula $e_{t-1}$
\end_inset

, where 
\begin_inset Formula $e_{t}=s_{t}-p_{t}$
\end_inset

.
 
\end_layout

\begin_layout Standard
LPCNet operates on signals quantized using 256-level 
\begin_inset Formula $\mu$
\end_inset

\SpecialChar nobreakdash
law.
 To avoid audible quantization noise we apply a pre-emphasis filter 
\begin_inset Formula $E(z)=1-\alpha z^{-1}$
\end_inset

 on the input speech (with 
\begin_inset Formula $\alpha=0.85$
\end_inset

) and the inverse (de-emphasis) filter on the output.
 This shapes the noise and makes it less perceptible.
 Considering that 
\begin_inset Formula $s_{t-1}$
\end_inset

, 
\begin_inset Formula $p_{t}$
\end_inset

, and 
\begin_inset Formula $e_{t-1}$
\end_inset

 are discrete, we can pre-compute the contribution 
\begin_inset Formula $\mathbf{v}_{i}^{\left(\cdot,\cdot\right)}$
\end_inset

 of each possible value to the 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 gates and state so that only lookups are necessary at run-time.
 In addition, the contribution 
\begin_inset Formula $\mathbf{g}^{\left(\cdot\right)}$
\end_inset

 of the frame rate network to 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 can be computed only once per frame.
 After these simplifications, only the recurrent matrices 
\begin_inset Formula $\mathbf{W}_{\left(\cdot\right)}$
\end_inset

 remain and the sample rate network is computed as
\begin_inset Formula 
\begin{align}
\mathbf{u}_{t}= & \sigma\left(\mathbf{W}_{u}\mathbf{h}_{t-1}+\mathbf{v}_{s_{t-1}}^{\left(u,s\right)}+\mathbf{v}_{p_{t}}^{\left(u,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(u,e\right)}+\mathbf{g}^{\left(u\right)}\right)\nonumber \\
\mathbf{r}_{t}= & \sigma\left(\mathbf{W}_{r}\mathbf{h}_{t-1}+\mathbf{v}_{s_{t-1}}^{\left(r,s\right)}+\mathbf{v}_{p_{t}}^{\left(r,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(r,e\right)}+\mathbf{g}^{\left(r\right)}\right)\label{eq:LPCNet}\\
\widetilde{\mathbf{h}}_{t}= & \tau\left(\mathbf{r}_{t}\circ\left(\mathbf{W}_{h}\mathbf{h}_{t-1}\right)+\mathbf{v}_{s_{t-1}}^{\left(h,s\right)}+\mathbf{v}_{p_{t}}^{\left(h,p\right)}+\mathbf{v}_{e_{t-1}}^{\left(h,e\right)}+\mathbf{g}^{\left(h\right)}\right)\nonumber \\
\mathbf{h}_{t}= & \mathbf{u}_{t}\circ\mathbf{h}_{t-1}+\left(1-\mathbf{u}_{t}\right)\circ\widetilde{\mathbf{h}}_{t}\nonumber \\
P\left(e_{t}\right) & =\mathrm{softmax}\left(\mathrm{dual\_fc}\left(\mathrm{GRU_{B}}\left(\mathbf{h}_{t}\right)\right)\right)\,,\nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $\sigma\left(x\right)$
\end_inset

 is the sigmoid function, 
\begin_inset Formula $\tau\left(x\right)$
\end_inset

 is the hyperbolic tangent, 
\begin_inset Formula $\circ$
\end_inset

 denotes an element-wise vector multiply, and 
\begin_inset Formula $\mathrm{GRU_{B}}\left(\cdot\right)$
\end_inset

 is a regular (non-sparse) GRU.
 The dual fully-connected (dual_fc) layer is defined as
\begin_inset Formula 
\begin{equation}
\mathrm{dual\_fc}(\mathbf{x})=\mathbf{a}_{1}\circ\tau\left(\mathbf{W}_{1}\mathbf{x}\right)+\mathbf{a}_{2}\circ\tau\left(\mathbf{W}_{2}\mathbf{x}\right)\,,\label{eq:dual_fc}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{W}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}_{2}$
\end_inset

 are weight matrices and 
\begin_inset Formula $\mathbf{a}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{a}_{2}$
\end_inset

 are scaling vectors.
\end_layout

\begin_layout Standard
Throughout this paper, biases are omitted for clarity.
 The synthesized excitation sample 
\begin_inset Formula $e_{t}$
\end_inset

 is obtained by sampling from the probability distribution 
\begin_inset Formula $P\left(e_{t}\right)$
\end_inset

 after lowering the 
\emph on
temperature
\emph default
 of voiced frames as described in eq.
\begin_inset space ~
\end_inset

(7) of 
\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

.
 As a way of reducing the complexity, 
\begin_inset Formula $\mathrm{GRU_{A}}$
\end_inset

 uses sparse recurrent matrices with non-zero blocks of size 16x1 to ensure
 efficient vectorization.
 Because the hidden state update is more important than the reset and update
 gates, we keep 20% of the weights in 
\begin_inset Formula $\mathbf{W}_{h}$
\end_inset

, but only 5% of those in 
\begin_inset Formula $\mathbf{W}_{r}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}_{u}$
\end_inset

, for an average of 10%.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename overview.pdf
	width 100col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the LPCNet model.
 The frame rate network (yellow) operates on 10\SpecialChar nobreakdash
ms frames and its output
 is held constant through each frame for the sample rate network (blue).
 The 
\emph on
compute prediction
\emph default
 block applies linear prediction to predict the sample at time 
\begin_inset Formula $t$
\end_inset

 from the previous samples.
 Conversions between 
\begin_inset Formula $\mu$
\end_inset

-law and linear are omitted for clarity.
 The de-emphasis filter is applied to the output 
\begin_inset Formula $s_{t}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-LPCNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Features and Quantization
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Features_and_quantization"

\end_inset


\end_layout

\begin_layout Standard
LPCNet is designed to operate with 10\SpecialChar nobreakdash
ms frames.
 Each frame includes 18
\begin_inset space ~
\end_inset

cepstral coefficients, a pitch period (between 16 and 256
\begin_inset space ~
\end_inset

samples), and a pitch correlation (between 0 and
\begin_inset space ~
\end_inset

1).
 To achieve a low bitrate, we use 40\SpecialChar nobreakdash
ms packets, each representing 4
\begin_inset space ~
\end_inset

LPCNet frames.
 Each packet is coded with 64
\begin_inset space ~
\end_inset

bits, allocated as shown in Table
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Bit-allocation"
plural "false"
caps "false"
noprefix "false"

\end_inset

, for a total bitrate of 1.6
\begin_inset space ~
\end_inset

kb/s (constant bitrate).
 The prediction coefficients are computed using the quantized cepstrum.
\end_layout

\begin_layout Standard
In addition to the packet size of 40
\begin_inset space ~
\end_inset

ms, LPCNet has a synthesis look-ahead of 25
\begin_inset space ~
\end_inset

ms: 20
\begin_inset space ~
\end_inset

ms for the two 1x3 convolutional layers in the frame rate network and 5
\begin_inset space ~
\end_inset

ms for the overlap in the analysis window (see Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:cepstrum"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The total algorithmic delay of the codec is thus 65
\begin_inset space ~
\end_inset

ms.
 Because the complexity of the frame-level processing is negligible compared
 to that of the sample rate network, no significant delay is added by the
 computation time (unlike traditional codecs where up to one frame delay
 can be added if the codec takes 100%
\begin_inset space ~
\end_inset

CPU).
\end_layout

\begin_layout Subsection
Pitch
\end_layout

\begin_layout Standard
Extracting the correct pitch (without period doubling or halving) is very
 important for a vocoder since no residual is coded to make up for prediction
 errors.
 During development, we have observed that unlike traditional vocoders,
 LPCNet has some ability to compensate for incorrect pitch values, but only
 up to a point.
 Moreover, that ability is reduced when the cepstrum is quantized.
 
\end_layout

\begin_layout Standard
The pitch search operates on the excitation signal.
 Maximizing the correlation over an entire 40\SpecialChar nobreakdash
ms packet does not produce
 good results because the pitch can vary within that time.
 Instead, we divide each packet in 5\SpecialChar nobreakdash
ms sub-frames, and find the set of pitch
 lags 
\begin_inset Formula $\tau_{i}$
\end_inset

 that maximize
\begin_inset Formula 
\begin{equation}
J=\sum_{i}\left[w_{i}r\left(\tau_{i}\right)-\Theta\left(\tau_{i}-\tau_{i-1}\right)\right]\,,\label{eq:pitch_cost}
\end{equation}

\end_inset

where 
\begin_inset Formula $w_{i}$
\end_inset

 is the ratio of the sub-frame 
\begin_inset Formula $i$
\end_inset

 energy over the average energy of the 40\SpecialChar nobreakdash
ms packet, 
\begin_inset Formula $\Theta\left(\Delta\tau\right)$
\end_inset

 is a transition penalty defined as 
\begin_inset Formula 
\begin{equation}
\Theta\left(\Delta\tau\right)=\begin{cases}
0.02\left(\Delta\tau\right)^{2} & \mathrm{if}\,\left|\Delta\tau\right|\leq4\\
6 & \mathrm{otherwise}
\end{cases}\,,\label{eq:pitch_transition}
\end{equation}

\end_inset

and 
\begin_inset Formula $r\left(\tau\right)$
\end_inset

 is a modified pitch correlation
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "vos2013voice"
literal "false"

\end_inset


\begin_inset Formula 
\begin{equation}
r\left(\tau\right)=\frac{2\sum_{n}e\left(n\right)e\left(n-\tau\right)}{\sum_{n}e^{2}\left(n\right)+\sum_{n}e^{2}\left(n-\tau\right)}\,.\label{eq:pitch_correlation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The optimal path can be computed efficiently using dynamic programming with
 a Viterbi search.
 Since the entire audio is not available at once, the values of 
\begin_inset Formula $J$
\end_inset

 in the forward pass are updated normally for every sub-frame but the Viterbi
 backtrack pass is computed once per 40\SpecialChar nobreakdash
ms packet, over all 8
\begin_inset space ~
\end_inset

sub-frames.
 While that does not guarantee finding the global optimal path, it ensures
 a consistent pitch over the duration of the packet, which is important
 for quantization.
\end_layout

\begin_layout Standard
The pitch is allowed to vary between 62.5
\begin_inset space ~
\end_inset

Hz and 500
\begin_inset space ~
\end_inset

Hz.
 The average pitch over the packet is encoded on a logarithmic scale using
 6
\begin_inset space ~
\end_inset

bits, resulting in quantization intervals of 0.57
\begin_inset space ~
\end_inset

semitones.
 
\end_layout

\begin_layout Standard
A linear pitch modulation parameter allows up to 16% variation (2.5
\begin_inset space ~
\end_inset

semitones) between the first and last sub-frame.
 It is encoded with 3 bits representing a 
\begin_inset Formula $\left[-3,3\right]$
\end_inset

 range with two different codes for 0 (constant modulation), one of which
 signaling that the pitch correlation is less than 0.3 (the modulation is
 zero when the pitch correlation is too small).
 Two extra bits refine the value of the pitch correlation within the remaining
 range (either 
\begin_inset Formula $\left[0,\,0.3\right]$
\end_inset

 or 
\begin_inset Formula $\left[0.3,\,1\right]$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bit allocation for a 40\SpecialChar nobreakdash
ms frame.
\begin_inset CommandInset label
LatexCommand label
name "tab:Bit-allocation"

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bits
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch period
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch modulation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pitch correlation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Energy (C0)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum VQ (40
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum delta (20
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cepstrum interpolation (10
\begin_inset space ~
\end_inset

ms)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Total
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cepstrum
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "subsec:cepstrum"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename quantization.pdf
	width 90col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Prediction and quantization of the cepstrum for packet
\begin_inset space ~
\end_inset


\begin_inset Formula $k$
\end_inset

.
 Vectors in green are quantized independently, vectors in blue are quantized
 with prediction, and vectors in red use prediction with no residual quantizatio
n.
 Prediction is shown by the arrows.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Prediction-and-quantization"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The spectral feature analysis operates on 20\SpecialChar nobreakdash
ms windows with a 10\SpecialChar nobreakdash
ms frame
 offset (50% overlap).
 The cepstrum is computed from 18
\begin_inset space ~
\end_inset

Bark-spaced bands following the same layout as
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2017hybrid"
literal "false"

\end_inset

.
 Because we pack 4
\begin_inset space ~
\end_inset

cepstral vectors in each packet, we wish to minimize redundancy within a
 packet while limiting dependencies across packets to reduce the effect
 of packet loss.
 For those reasons, we use a prediction scheme (Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Prediction-and-quantization"
plural "false"
caps "false"
noprefix "false"

\end_inset

) inspired by video codec B\SpecialChar nobreakdash
frames
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "MPEG1"
literal "false"

\end_inset

, limiting the error propagation in case of packet loss to a worst case
 of 30
\begin_inset space ~
\end_inset

ms.
\end_layout

\begin_layout Standard
Let packet 
\begin_inset Formula $k$
\end_inset

 include cepstral vectors 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 to 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 We start by coding the first component (C0) of 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

 independently, with a uniform 7\SpecialChar nobreakdash
bit scalar quantizer (0.83
\begin_inset space ~
\end_inset

dB resolution).
 We then code the remaining component of 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

 independently using a 3\SpecialChar nobreakdash
stage codebook with 17
\begin_inset space ~
\end_inset

dimensions and 10
\begin_inset space ~
\end_inset

bits for each stage.
 An M-best (survivor) search helps reduce the quantization error, but is
 not strictly necessary.
 From there, vector 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

 is predictively coded using both 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 (independently coded in the previous packet) and 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 We use a single bit to signal if the prediction is the average (
\begin_inset Formula $\frac{\mathbf{c}_{4k-1}+\mathbf{c}_{4k+3}}{2}$
\end_inset

), or two bits if the prediction is either of 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 or 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 The 18\SpecialChar nobreakdash
dimensional prediction residual is then coded with a 11\SpecialChar nobreakdash
bit + sign
 codebook for the average predictor or with an 10\SpecialChar nobreakdash
bit + sign codebook if
 not, for a total of 13
\begin_inset space ~
\end_inset

bits for 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

.
 Although the average predictor is the most useful, including the single-vector
 predictors improves the quantization of transients/onsets.
 
\end_layout

\begin_layout Standard
Because there is insufficient bitrate to adequately code 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+2}$
\end_inset

, we only use a prediction from their neighbors.
 Vector 
\begin_inset Formula $\mathbf{c}_{4k}$
\end_inset

 is predicted from its neighbors 
\begin_inset Formula $\mathbf{c}_{4k-1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

, whereas 
\begin_inset Formula $\mathbf{c}_{4k+2}$
\end_inset

 is predicted from 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{4k+3}$
\end_inset

.
 Since there are 3
\begin_inset space ~
\end_inset

options for each vector, we have 9
\begin_inset space ~
\end_inset

possible combinations.
 By eliminating the least useful combination (forcing both equal to 
\begin_inset Formula $\mathbf{c}_{4k+1}$
\end_inset

), we can code the remaining ones with 3
\begin_inset space ~
\end_inset

bits.
\end_layout

\begin_layout Section
Training
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Training"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename training_noise.pdf
	width 100col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Noise injection during the training procedure, with 
\begin_inset Formula $Q$
\end_inset

 denoting 
\begin_inset Formula $\mu$
\end_inset

-law quantization and 
\begin_inset Formula $Q^{-1}$
\end_inset

 denoting conversion from 
\begin_inset Formula $\mu$
\end_inset

-law to linear.
 The prediction filter filter is given by 
\begin_inset Formula $P\left(z\right)=\sum_{i=1}^{M}a_{i}z^{-k}$
\end_inset

.
 The target excitation is computed as the difference between the clean,
 unquantized input and the noisy prediction.
 Note that the noise is added in the 
\begin_inset Formula $\mu$
\end_inset

-law domain so that its power follows that of the real excitation signal.
\begin_inset CommandInset label
LatexCommand label
name "fig:Noise-injection"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When decoding speech, the sample rate network operates on the synthesized
 speech rather than on the 
\emph on
ground-truth
\emph default
 clean speech.
 That mismatch between training and inference can increase the speech distortion
, in turn increasing the mismatch.
 To make the network more robust to the mismatch, we add noise to the training
 data, as originally suggested in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "jin2018fftnet"
literal "false"

\end_inset

.
 As an improvement over our previous work
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

, Laplace-distributed noise is added to the excitation inside the prediction
 loop, making the signal and excitation noise consistent with each other,
 as shown in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Noise-injection"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
As a way of making the model more robust to input variations, we use data
 augmentation on the training database.
 The speech level is varied over a 40
\begin_inset space ~
\end_inset

dB range and the frequency response is varied according to eq.
\begin_inset space ~
\end_inset

(7) in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2017hybrid"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Training is first performed with unquantized features.
 When training a model for quantized features, we start with a model trained
 on unquantized features, and apply domain adaptation to quantized features.
 We have observed that better results are obtained when only the frame rate
 network is adapted, with the sample rate network weights left unchanged.
 In addition to the slightly better quality, this has the advantage of faster
 training for new quantizers and also smaller storage if different quantized
 models are needed.
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Evaluation"

\end_inset


\end_layout

\begin_layout Standard
The source code for this work is available under a BSD license at 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://github.com/mozilla/LPCNet/}
\end_layout

\end_inset

.
 The evaluation in this section is based on commit 
\family typewriter
343e35f
\family default
.
\end_layout

\begin_layout Subsection
Complexity and Implementation
\end_layout

\begin_layout Standard
The number of weights in the sample rate network is approximately
\begin_inset Formula 
\begin{equation}
W=3dN_{A}^{2}+3N_{B}\left(N_{A}+N_{B}\right)+2N_{B}Q\,,\label{eq:weight_count}
\end{equation}

\end_inset

where 
\begin_inset Formula $N_{A}$
\end_inset

 and 
\begin_inset Formula $N_{B}=16$
\end_inset

 are the sizes of the two GRUs, 
\begin_inset Formula $d$
\end_inset

 is the density of the sparse GRU, 
\begin_inset Formula $Q=256$
\end_inset

 is the number of 
\begin_inset Formula $\mu$
\end_inset

-law levels.
 Based on the subjective results in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lpcnet"
literal "false"

\end_inset

, we consider 
\begin_inset Formula $N_{A}=384$
\end_inset

 with 
\begin_inset Formula $d=10\%$
\end_inset

 (122
\begin_inset space ~
\end_inset

dense equivalent units) to provide a good compromise between quality and
 complexity.
 This results in 
\begin_inset Formula $W=71600\,\mathrm{weights}$
\end_inset

, which fits in the L2 or L3 cache of most modern CPUs.
 Considering that each weight is used once per sample for a multiply-add,
 the resulting complexity is 2.3
\begin_inset space ~
\end_inset

GFLOPS.
 The activation functions are based on a vectorized exponential approximation
 and contribute 0.6
\begin_inset space ~
\end_inset

GFLOPS to the complexity, for a total of 3
\begin_inset space ~
\end_inset

GFLOPS when counting the remaining operations.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
exp=15, tanh=19, sig=18, softmax=21
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A C implementation of the decoder (with AVX2/FMA intrinsics) requires 20%
 of a 2.4
\begin_inset space ~
\end_inset

GHz Intel Broadwell core for real-time decoding (5x faster than real-time).
 According to our analysis, the main performance bottleneck is the L2 cache
 bandwidth required for the matrix-vector products.
 On ARMv8 (with Neon intrinsics), real-time decoding on a 2.5
\begin_inset space ~
\end_inset

GHz Snapdragon
\begin_inset space ~
\end_inset

845 (Google Pixel
\begin_inset space ~
\end_inset

3) requires 68% of one core (1.47x
\begin_inset space ~
\end_inset

real-time).
 On the more recent 2.84
\begin_inset space ~
\end_inset

GHz Snapdragon
\begin_inset space ~
\end_inset

855 (Samsung Galaxy
\begin_inset space ~
\end_inset

S10), real-time decoding requires only 31% of one core (3.2x
\begin_inset space ~
\end_inset

real-time).
\end_layout

\begin_layout Standard
As a comparison, we estimate the complexity of the SampleRNN neural codec
 described in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

 to be around 100
\begin_inset space ~
\end_inset

GFLOPS – mostly from the MLP with two hiddens layers and 1024 units per
 layer.
 The complexity of the WaveNet-based codec in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kleijn2018wavenet"
literal "false"

\end_inset

 significantly exceeds 100
\begin_inset space ~
\end_inset

GFLOPS
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
based on discussion with the authors
\end_layout

\end_inset

.
 For speaker-dependent text-to-speech
\begin_inset space ~
\end_inset

(TTS) – which typically allows smaller models – real-time synthesis was
 achieved by
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "arik2017deep_rt"
literal "false"

\end_inset

 (WaveNet) and
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kalchbrenner2018efficient"
literal "false"

\end_inset

 (WaveRNN) when using multiple CPU cores.
\end_layout

\begin_layout Standard
The total complexity of the proposed encoder is around 30
\begin_inset space ~
\end_inset

MFLOPS (0.03
\begin_inset space ~
\end_inset

GFLOPS), mostly from the 5-best VQ search (14
\begin_inset space ~
\end_inset

MFLOPS) and the undecimated 16
\begin_inset space ~
\end_inset

kHz pitch search (8
\begin_inset space ~
\end_inset

MFLOPS).
 Although the encoder complexity could be significantly reduced, it is already
 only 1% of the decoder complexity.
\end_layout

\begin_layout Subsection
Experimental Setup
\end_layout

\begin_layout Standard
The model is trained using 4
\begin_inset space ~
\end_inset

hours of speech from the NTT Multi-Lingual Speech Database for Telephonometry
 (21
\begin_inset space ~
\end_inset

languages), from which we excluded all samples from the speakers used in
 testing.
 From the original data, we generate 14
\begin_inset space ~
\end_inset

hours of augmented speech data as described in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Training"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The unquantized network was trained for 120
\begin_inset space ~
\end_inset

epochs (625k
\begin_inset space ~
\end_inset

updates), with a batch size size of 64, each sequence consisting of 2400
\begin_inset space ~
\end_inset

samples (15
\begin_inset space ~
\end_inset

frames).
 Training was performed on an Nvidia GPU with Keras
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://keras.io/}
\end_layout

\end_inset


\end_layout

\end_inset

/Tensorflow
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://www.tensorflow.org/}
\end_layout

\end_inset


\end_layout

\end_inset

 using the CuDNN GRU implementation and the AMSGrad
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "reddi2018convergence"
literal "false"

\end_inset

 optimization method (Adam variant) with a step size 
\begin_inset Formula $\alpha=\frac{\alpha_{0}}{1+\delta\cdot b}$
\end_inset

 where 
\begin_inset Formula $\alpha_{0}=0.001$
\end_inset

, 
\begin_inset Formula $\delta=5\times10^{-5}$
\end_inset

, and 
\begin_inset Formula $b$
\end_inset

 is the batch number.
 For model adaptation with quantized features, we used 40
\begin_inset space ~
\end_inset

epochs (208k
\begin_inset space ~
\end_inset

updates) with 
\begin_inset Formula $\alpha_{0}=0.0001$
\end_inset

, 
\begin_inset Formula $\delta=0$
\end_inset

.
\end_layout

\begin_layout Subsection
Quality
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering{
\end_layout

\end_inset


\begin_inset Graphics
	filename mushra1.pdf
	width 100col%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Subjective quality (MUSHRA) results for both listening tests.
 Set
\begin_inset space ~
\end_inset

1 is taken from the NTT database, while Set 2 consists of Opus testvector
 samples.
\begin_inset CommandInset label
LatexCommand label
name "fig:Subjective-quality-MUSHRA"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We conducted a subjective listening test with a MUSHRA-inspired methodology
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "BS1534"
literal "false"

\end_inset

 to evaluate the quality of the proposed 1.6
\begin_inset space ~
\end_inset

kb/s neural vocoder.
 As an upper bound on the quality achievable with LPCNet at the target complexit
y (higher quality is achievable with a larger model), we include LPCNet
 operating on unquantized features.
 We also compare with Opus
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "rfc6716"
literal "false"

\end_inset

 wideband (SILK mode) operating at 9
\begin_inset space ~
\end_inset

kb/s VBR
\begin_inset Foot
status open

\begin_layout Plain Layout
the lowest bitrate for which the encoder defaults to wideband
\end_layout

\end_inset

 and with the narrowband MELP
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "mccree1996"
literal "false"

\end_inset

 vocoder.
 As low anchor, we use Speex
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "valin2007speex"
literal "false"

\end_inset

 operating as a 4
\begin_inset space ~
\end_inset

kb/s wideband vocoder (wideband quality
\begin_inset space ~
\end_inset

0).
\end_layout

\begin_layout Standard
In a first test (Set
\begin_inset space ~
\end_inset

1), we used 8
\begin_inset space ~
\end_inset

samples from 2
\begin_inset space ~
\end_inset

male and 2
\begin_inset space ~
\end_inset

female speakers.
 The samples are part of the NTT database used for training, but all samples
 from the selected speakers for the test were excluded from the training
 set.
 As reported in
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "klejsa2018high"
literal "false"

\end_inset

, mismatches between the training and testing database can cause a significant
 difference in the output quality.
 We measure that impact in a second test (Set
\begin_inset space ~
\end_inset

2) on the same model, with 8
\begin_inset space ~
\end_inset

samples (one
\begin_inset space ~
\end_inset

male and one
\begin_inset space ~
\end_inset

female speaker) from the sample set used to create the Opus testvectors.
 Each test included 100
\begin_inset space ~
\end_inset

listeners.
 
\end_layout

\begin_layout Standard
The results in Fig.
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Subjective-quality-MUSHRA"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show that 1.6
\begin_inset space ~
\end_inset

kb/s LPCNet clearly outperforms the MELP vocoder, making it a viable choice
 for very low bitrates.
 The fact that LPCNet with unquantized features achieves slightly higher
 quality than Opus at 9
\begin_inset space ~
\end_inset

kb/s suggests that LPCNet at bitrates around 2-6
\begin_inset space ~
\end_inset

kb/s may be able to compete with waveform coders below 10
\begin_inset space ~
\end_inset

kb/s.
 The test with samples outside the NTT database shows that the LPCNet model
 generalizes to other recording conditions.
 
\end_layout

\begin_layout Standard
Since the LPCNet model was trained on 21
\begin_inset space ~
\end_inset

languages, it is expected to also work in those languages.
 While we only tested on English, informal listening indicates that the
 quality obtained on French, Spanish, and Swedish is comparable to that
 on English.
\end_layout

\begin_layout Standard
A subset of the samples from the listening test is available at 
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
footnotesize
\backslash
url{https://people.xiph.org/~jm/demo/lpcnet_codec/}}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\end_layout

\begin_layout Standard
We have demonstrated a 1.6
\begin_inset space ~
\end_inset

kb/s neural vocoder based on the LPCNet model that can be used for real-time
 communication on a mobile device.
 The quality obtained exceeds what is achievable with existing low-bitrate
 vocoders such as MELP.
 Although other work has demonstrated similar or higher speech quality at
 similar bitrates
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kleijn2018wavenet,Garbacea2019,klejsa2018high"
literal "false"

\end_inset

, we believe this is the first neural vocoder that can operate in real-time
 on general-purpose hardware and mobile devices.
 We have so far focused on clean, non-reverberant speech.
 More work is needed for testing and improving the robustness to noise and
 reverberation.
 
\end_layout

\begin_layout Standard
Considering that uncompressed LPCNet is able to achieve higher quality than
 9
\begin_inset space ~
\end_inset

kb/s Opus, we believe it is worth exploring higher LPCNet bitrates in the
 2\SpecialChar nobreakdash
6
\begin_inset space ~
\end_inset

kb/s range.
 Moreover, in the case of a waveform codec like Opus operating at very low
 bitrate (<
\begin_inset space ~
\end_inset

8kb/s) it should be possible to directly use the encoded LSPs to synthesize
 a higher quality output than the standard provides.
 In addition, using the encoded excitation as an extra input to LPCNet may
 help further reducing artifacts, turning LPCNet into a neural post-filter
 that can significantly improve the quality of any existing low-bitrate
 waveform codec.
\end_layout

\begin_layout Section
Acknowledgements
\end_layout

\begin_layout Standard
We thank David Rowe for his feedback and suggestions, as well as for implementin
g some of the Neon optimizations.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "lpcnet"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
